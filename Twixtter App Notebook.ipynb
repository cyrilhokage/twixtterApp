{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining data on twitter with python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this article: https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "- TWEEPY 3.7\n",
    "- nltk\n",
    "- re\n",
    "- json\n",
    "- operator\n",
    "- collections\n",
    "- string\n",
    "- vincent\n",
    "- pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vincent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Collecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACCESS_TOKEN': '720396530759385088-8bQxnzGDXXGHQB63eumuFYWnmL4nF6H',\n",
       " 'ACCESS_TOKEN_SECRET': 'by2vprExn9FF4Sg2QJupvpZqhyiVYBr62FjB2VhkF0esv',\n",
       " 'CONSUMER_KEY': 'RRZFSHS1Nw8xH4eRyBaftyFfh',\n",
       " 'CONSUMER_SECRET': 'xdw3qoO1OmwZsb95CpDNpY4X7nYOwwgGeAPN2y6oiI9xP6Xkbx'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = credentials['CONSUMER_KEY'] \n",
    "consumer_secret = credentials['CONSUMER_SECRET']\n",
    "access_token = credentials['ACCESS_TOKEN']\n",
    "access_token_secret = credentials['ACCESS_TOKEN_SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_tweets = api.home_timeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @ValetteClaude: Je veux des vid√©os de vous entrain de chanter.\n",
      "RT @ubereats_fr: La fille est Arya Stark de Winterfell. Et je commande pour ma maison ! \n",
      "#GameOfThrones en exclusivit√© sur @OCSTV. https://‚Ä¶\n",
      "RT @Jarrynette34: Quand en soiree tu veux pas quon te pique ton verre ü§£ #VTEP https://t.co/ntoqXNXz7c\n",
      "RT @CORNEVILLE: Ou va t'il chercher ces nouvelles  epreuves @Arthur_Officiel  #VTEP\n",
      "RT @boytoygone: #VTEP excellent depuis le temps qu'il fallait des nouvelles √©preuves top mDr üòÖüòÖüòÖ\n",
      "RT @soldatguerrier1: Je Kiffe le 360 c'est la meilleure invention @Arthur_Officiel bravo aux √©quipes de @Satisfaction_tv üëèüëè #VTEP #TF1\n",
      "RT @08_rosemary03: La magie de #VTEP avec @Arthur_Officiel et tous les invit√©s et surtout @CartmanOfficiel et @cyrilferaud ‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§\n",
      "RT @mauritiusgirly: Trop balaise la piece ou ont se retrouve a lenvers ü§£ü§£ #VTEP\n",
      "RT @Rodrigu64581700: L'accident √ßa nous fera un peu le buzz üòÇüòÇüòÇ @Arthur_Officiel t'es un enfoir√© üòÇüòÇüòÇ #VTEP\n",
      "RT @chriiistine__75: Ah oui, de nouveau le plateau a 360 et tout √ßa en dansant üòèüëåüèªüòÇ #vtep\n",
      "RT @sanaa___kg: Mdrrr la t√™te de Ryad üòÇüòÇ#VTEP\n",
      "RT @ophelieavgc: pauvre @justriadh üòÇ #VTEP\n",
      "RT @YNadoore: Riadh dans #VTEP üò≠\n",
      "RT @Mariettegalaxy: je suis deadü§£ü§£ü§£ü§£ü§£ #VTEP\n",
      "RT @Rodrigu64581700: Vous √™tes m√©chants le pauvre  üòÇüòÇüòÇ #VTEP\n",
      "RT @shaqbrewster: .@BernieSanders had to move his South Carolina event indoors due to bad weather, so he addressed an overflow crowd outsid‚Ä¶\n",
      "RT @Jennifer1382: L'√©preuve du mime en apesanteur est absolument g√©niale, il faut la garder üòÅ #VTEP\n",
      "RT @mauritiusgirly: C'est pas facile de faire des mimes suspendu üòÇüòÇ le d√©lire  #VTEP\n",
      "RT @soldatguerrier1: excellent #VTEP ce soir merci pour toutes ses nouvelles √©preuves @Arthur_Officiel. @VTEP_TF1 #TF1\n",
      "RT @Rodrigu64581700: Mais qui invente ces √©preuves üòÇüòÇüòÇ je l'embrasse üòòüòò #vtep üôè\n"
     ]
    }
   ],
   "source": [
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in tweepy.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    status_list.append(status._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a streaming listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('python.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch a real time listener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['#python'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n"
     ]
    }
   ],
   "source": [
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in status_list:\n",
    "    tokens = preprocess(status['text'])\n",
    "    tokens_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@rocmaelias',\n",
       " ':',\n",
       " 'Jvien',\n",
       " 'de',\n",
       " 'd√©couvrir',\n",
       " 'que',\n",
       " 'le',\n",
       " 'ptit',\n",
       " 'truc',\n",
       " 'jaune',\n",
       " \"c'est\",\n",
       " 'comme',\n",
       " 'le',\n",
       " 'jaune',\n",
       " 'de',\n",
       " \"l'oeuf\",\n",
       " 'https://t.co/UNDANnNc6y']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Term Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('french') + punctuation + ['RT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count term occurences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('jaune', 2), ('Jvien', 1), ('d√©couvrir', 1), ('ptit', 1), ('truc', 1)]\n",
      "[('jaune', 2), ('Jvien', 1), ('d√©couvrir', 1), ('ptit', 1), ('truc', 1)]\n",
      "[('jaune', 2), ('Jvien', 1), ('d√©couvrir', 1), ('ptit', 1), ('truc', 1)]\n",
      "[('jaune', 2), ('veux', 2), ('Jvien', 1), ('d√©couvrir', 1), ('ptit', 1)]\n",
      "[('jaune', 2), ('veux', 2), ('Jvien', 1), ('d√©couvrir', 1), ('ptit', 1)]\n",
      "[('üòÖ', 3), ('jaune', 2), ('veux', 2), ('nouvelles', 2), ('Jvien', 1)]\n",
      "[('üòÖ', 3), ('jaune', 2), (\"c'est\", 2), ('Je', 2), ('veux', 2)]\n",
      "[('‚ù§', 6), ('üòÖ', 3), ('jaune', 2), (\"c'est\", 2), ('Je', 2)]\n",
      "[('‚ù§', 6), ('ü§£', 3), ('üòÖ', 3), ('jaune', 2), (\"c'est\", 2)]\n",
      "[('‚ù§', 6), ('üòÇ', 6), ('ü§£', 3), ('üòÖ', 3), ('jaune', 2)]\n"
     ]
    }
   ],
   "source": [
    "count_all = Counter()\n",
    "for status in status_list:\n",
    "    \n",
    "    # Create a list with all the terms\n",
    "    terms_stop = [term for term in preprocess(status['text']) if term not in stop and not term.startswith(('#', '@'))]\n",
    "    \n",
    "    terms_hash = [term for term in preprocess(status['text']) if term.startswith('#')]\n",
    "    \n",
    "    # Update the counter\n",
    "    count_all.update(terms_stop)\n",
    "    \n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_bigram = bigrams(terms_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"L'accident\", '√ßa'), ('√ßa', 'fera'), ('fera', 'peu'), ('peu', 'buzz'), ('buzz', 'üòÇ'), ('üòÇ', 'üòÇ'), ('üòÇ', 'üòÇ'), ('üòÇ', \"t'es\"), (\"t'es\", 'enfoir'), ('enfoir', '√©'), ('√©', 'üòÇ'), ('üòÇ', 'üòÇ'), ('üòÇ', 'üòÇ')]\n"
     ]
    }
   ],
   "source": [
    "print(list(terms_bigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Term Co-occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will stream recents tweets about this last European Champion's Leagues matchs played this week.\n",
    "So, will use the stream function to stream the #UCL ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch a real time listener on #UCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '#UCL'\n",
    "max_tweets = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_tweets = [status for status in tweepy.Cursor(api.search, q=query).items(max_tweets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save tweets in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ucl_tweets.json', 'w') as outfile:\n",
    "    for status in searched_tweets:\n",
    "        json.dump(status._json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "searched_tweets[0]._json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import english stop word and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('french') + punctuation + ['RT', 'Ô∏è', '‚Ä¶', '', ] + stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count term occurences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all = Counter()\n",
    "for status in searched_tweets:\n",
    "    \n",
    "    # Create a list with all the terms\n",
    "    terms_stop = [term for term in preprocess(status._json['text']) if term not in stop and not term.startswith(('#', '@')) and not term.isdigit()]\n",
    "    \n",
    "    # Update the counter\n",
    "    count_all.update(terms_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Messi', 328), ('‚öΩ', 304), ('üèÜ', 243), ('Week', 155), ('Goal', 135), ('Sterling', 130), ('Lionel', 126), ('David', 126), ('Neres', 125), ('Raheem', 124), ('Philippe', 124), ('Coutinho', 124), ('ü§î', 117), ('Agree', 117), ('ord', 117), ('ü•á', 98), ('üóì', 93), ('Majestic', 92), ('scoops', 92), ('prize', 92), ('https://t.co/L2qzAL39wg', 92), ('üòÇ', 88), ('\\U0001f929', 84), ('us', 81), ('‚Äô', 81), ('If', 80), ('nowt', 78), ('else', 78), ('given', 78), ('priceless', 78), ('moments', 78), ('How', 78), ('https://t.co/pChlaUsJG4', 78), ('Redondo', 77), ('üòç', 76), ('amp', 68), ('best', 65), ('assists', 64), ('history', 64), ('üî•', 64)]\n"
     ]
    }
   ],
   "source": [
    "# Print the most frequent words\n",
    "print(count_all.most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all = Counter()\n",
    "for status in searched_tweets:\n",
    "    \n",
    "    # Create a list with all the terms\n",
    "    terms_hash = [term for term in preprocess(status._json['text']) if term not in stop and term.startswith(('#')) and not term.isdigit()]\n",
    "    \n",
    "    # Update the counter\n",
    "    count_all.update(terms_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#UCL', 696), ('#POTW', 99), ('#OnThisDay', 85), ('#VAR', 80), ('#FlashbackFriday', 51), ('#COYS', 40), ('#GOTW', 23), ('#ajabay', 20), ('#LFC', 19), ('#FCPorto', 17), ('#SomosPorto', 17), ('#Ajax', 17), ('#TalD√≠aComoHoy', 16), ('#juvaja', 11), ('#MCITOT', 11), ('#„Ç¶„Ç§„Ç§„É¨', 8), ('#„Ç¶„Ç§„Ç§„É¨„Ç¢„Éó„É™', 8), ('#ChampionsLeagu', 8), ('#ucl', 8), ('#UEL', 6), ('#MUFC', 6), ('#JuveAjax', 6), ('#OTD', 4), ('#ChampionsLeague', 4), ('#TICKETMANIA', 4), ('#Datos', 4), ('#ChampionsLea', 4), ('#Messi', 3), ('#FIFA19', 3), ('#≈†kriniar', 2), ('#Eintracht', 2), ('#totaja', 2), ('#LdC', 2), ('#YNWA', 2), ('#GETREADY', 2), ('#LaLiga', 2), ('#DrakeCurse', 2), ('#barcelona', 2), ('#Guardiola', 2), ('#ajajuv', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Print the most frequent hashtags\n",
    "print(count_all.most_common(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Term co-occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = defaultdict(lambda : defaultdict(int))\n",
    " \n",
    "# f is the file pointer to the JSON data set\n",
    "for status in searched_tweets: \n",
    "    \n",
    "    terms_only = [term for term in preprocess(status._json['text']) \n",
    "                  if term not in stop \n",
    "                  and not term.startswith(('#', '@'))\n",
    "                  and not term.isdigit()]\n",
    " \n",
    "    # Build co-occurrence matrix\n",
    "    for i in range(len(terms_only)-1):            \n",
    "        for j in range(i+1, len(terms_only)):\n",
    "            w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "            if w1 != w2:\n",
    "                com[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_max = []\n",
    "# For each term, look for the most common co-occurrent terms\n",
    "for t1 in com:\n",
    "    t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    for t2, t2_count in t1_max_terms:\n",
    "        com_max.append(((t1, t2), t2_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('‚öΩ', 'üèÜ'), 472), (('Messi', '‚öΩ'), 271), (('‚öΩ', 'ü§î'), 234), (('Goal', '‚öΩ'), 234), (('Goal', 'üèÜ'), 234)]\n"
     ]
    }
   ],
   "source": [
    "# Get the most frequent co-occurrences\n",
    "terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "print(terms_max[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate a term co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word = \"Messi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_search = Counter()\n",
    "for status in searched_tweets:\n",
    "    terms_only = [term for term in preprocess(status._json['text']) \n",
    "                  if term not in stop \n",
    "                  and not term.startswith(('#', '@'))\n",
    "                  and not term.isdigit()]\n",
    "    if search_word in terms_only:\n",
    "        count_search.update(terms_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence for Messi:\n",
      "[('Messi', 328), ('‚öΩ', 271), ('üèÜ', 234), ('Week', 152), ('Goal', 135), ('Lionel', 126), ('Raheem', 124), ('Sterling', 124), ('David', 124), ('Neres', 124), ('Philippe', 124), ('Coutinho', 124), ('ü§î', 117), ('Agree', 117), ('ord', 117), ('ü•á', 98), ('Majestic', 92), ('scoops', 92), ('prize', 92), ('https://t.co/L2qzAL39wg', 92)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Co-occurrence for %s:\" % search_word)\n",
    "print(count_search.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Visualisation Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = count_all.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, freq = zip(*most_common_words)\n",
    "data = {'data': freq, 'x': labels}\n",
    "bar = vincent.Bar(data, iter_idx='x')\n",
    "bar.to_json('term_freq.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar.to_json('term_freq.json', html_out=True, html_path='chart.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_UCL = []\n",
    "\n",
    "for status in searched_tweets:\n",
    "    \n",
    "    # let's focus on hashtags only at the moment\n",
    "    terms_hash = [term for term in preprocess(status._json['text']) if term.startswith('#')]\n",
    "    # track when the hashtag is mentioned\n",
    "    if '#UCL' in terms_hash:\n",
    "        dates_UCL.append(status._json['created_at'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of \"1\" to count the hashtags\n",
    "ones = [1]*len(dates_UCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ones) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the index of the series\n",
    "idx = pd.DatetimeIndex(dates_UCL)\n",
    "# the actual series (at series of 1s for the moment)\n",
    "UCL = pd.Series(ones, index=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling / bucketing\n",
    "per_minute = UCL.resample('1Min').sum().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the time series on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chart = vincent.Line(per_minute)\n",
    "time_chart.axis_titles(x='Time', y='Freq')\n",
    "time_chart.to_json('time_chart.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather time-seres data from other hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get timeSeries data on 1 hastag given in parameter,\n",
    "#  the other parameter is the tweet's source ( a tweepy's cursor collection)\n",
    "# return a dataframe with time-series (grouped by minutes) as index\n",
    "def get_time_series(hastag, searched_tweets):\n",
    "    \n",
    "    dates = []\n",
    "\n",
    "    for status in searched_tweets:\n",
    "    \n",
    "        # let's focus on hashtags only at the moment\n",
    "        terms_hash = [term for term in preprocess(status._json['text'])]\n",
    "        # track when the hashtag is mentioned\n",
    "        if hastag in terms_hash:\n",
    "            dates.append(status._json['created_at'])\n",
    "            \n",
    "    # a list of \"1\" to count the hashtags\n",
    "    ones = [1]*len(dates)\n",
    "    \n",
    "    # the index of the series\n",
    "    idx = pd.DatetimeIndex(dates)\n",
    "    # the actual series (at series of 1s for the moment)\n",
    "    df_hashtag = pd.Series(ones, index=idx)\n",
    "    \n",
    "    # Resampling / bucketing\n",
    "    per_minute = df_hashtag.resample('1Min').sum().fillna(0)\n",
    "    \n",
    "    return per_minute\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get time-Series data for 'Messi', 'Coutinho' and '‚öΩ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_minute_messi = get_time_series('Messi', searched_tweets)\n",
    "per_minute_cou = get_time_series('Coutinho', searched_tweets)\n",
    "per_minute_ball = get_time_series('‚öΩ', searched_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all these data on the same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the data together\n",
    "tweets_data = dict(MESSI=per_minute_messi, COU=per_minute_cou, BALL=per_minute_ball)\n",
    "# we need a DataFrame, to accommodate multiple series\n",
    "all_matches = pd.DataFrame(data=tweets_data,\n",
    "                               index=per_minute_messi.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling as above\n",
    "#all_matches = all_matches.resample('1Min').sum().fillna(0)\n",
    " \n",
    "# and now the plotting\n",
    "time_chart = vincent.Line(all_matches[['COU', 'MESSI']])\n",
    "time_chart.axis_titles(x='Time', y='Freq')\n",
    "time_chart.legend(title='Matches')\n",
    "time_chart.to_json('multi_times_chart.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Messi', 328), ('‚öΩ', 304), ('üèÜ', 243), ('Week', 155), ('Goal', 135), ('Sterling', 130), ('Lionel', 126), ('David', 126), ('Neres', 125), ('Raheem', 124), ('Philippe', 124), ('Coutinho', 124), ('ü§î', 117), ('Agree', 117), ('ord', 117), ('ü•á', 98), ('üóì', 93), ('Majestic', 92), ('scoops', 92), ('prize', 92), ('https://t.co/L2qzAL39wg', 92), ('üòÇ', 88), ('\\U0001f929', 84), ('us', 81), ('‚Äô', 81), ('If', 80), ('nowt', 78), ('else', 78), ('given', 78), ('priceless', 78), ('moments', 78), ('How', 78), ('https://t.co/pChlaUsJG4', 78), ('Redondo', 77), ('üòç', 76), ('amp', 68), ('best', 65), ('assists', 64), ('history', 64), ('üî•', 64)]\n"
     ]
    }
   ],
   "source": [
    "# Print the most frequent words\n",
    "print(count_all.most_common(40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
