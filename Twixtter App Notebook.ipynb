{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining data on twitter with python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this article: https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "- TWEEPY 3.7\n",
    "- nltk\n",
    "- re\n",
    "- json\n",
    "- operator\n",
    "- collections\n",
    "- string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Collecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACCESS_TOKEN': '720396530759385088-8bQxnzGDXXGHQB63eumuFYWnmL4nF6H',\n",
       " 'ACCESS_TOKEN_SECRET': 'by2vprExn9FF4Sg2QJupvpZqhyiVYBr62FjB2VhkF0esv',\n",
       " 'CONSUMER_KEY': 'RRZFSHS1Nw8xH4eRyBaftyFfh',\n",
       " 'CONSUMER_SECRET': 'xdw3qoO1OmwZsb95CpDNpY4X7nYOwwgGeAPN2y6oiI9xP6Xkbx'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = credentials['CONSUMER_KEY'] \n",
    "consumer_secret = credentials['CONSUMER_SECRET']\n",
    "access_token = credentials['ACCESS_TOKEN']\n",
    "access_token_secret = credentials['ACCESS_TOKEN_SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_tweets = api.home_timeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @ValetteClaude: Je veux des vidÃ©os de vous entrain de chanter.\n",
      "RT @ubereats_fr: La fille est Arya Stark de Winterfell. Et je commande pour ma maison ! \n",
      "#GameOfThrones en exclusivitÃ© sur @OCSTV. https://â€¦\n",
      "RT @Jarrynette34: Quand en soiree tu veux pas quon te pique ton verre ğŸ¤£ #VTEP https://t.co/ntoqXNXz7c\n",
      "RT @CORNEVILLE: Ou va t'il chercher ces nouvelles  epreuves @Arthur_Officiel  #VTEP\n",
      "RT @boytoygone: #VTEP excellent depuis le temps qu'il fallait des nouvelles Ã©preuves top mDr ğŸ˜…ğŸ˜…ğŸ˜…\n",
      "RT @soldatguerrier1: Je Kiffe le 360 c'est la meilleure invention @Arthur_Officiel bravo aux Ã©quipes de @Satisfaction_tv ğŸ‘ğŸ‘ #VTEP #TF1\n",
      "RT @08_rosemary03: La magie de #VTEP avec @Arthur_Officiel et tous les invitÃ©s et surtout @CartmanOfficiel et @cyrilferaud â¤â¤â¤â¤â¤â¤\n",
      "RT @mauritiusgirly: Trop balaise la piece ou ont se retrouve a lenvers ğŸ¤£ğŸ¤£ #VTEP\n",
      "RT @Rodrigu64581700: L'accident Ã§a nous fera un peu le buzz ğŸ˜‚ğŸ˜‚ğŸ˜‚ @Arthur_Officiel t'es un enfoirÃ© ğŸ˜‚ğŸ˜‚ğŸ˜‚ #VTEP\n",
      "RT @chriiistine__75: Ah oui, de nouveau le plateau a 360 et tout Ã§a en dansant ğŸ˜ğŸ‘ŒğŸ»ğŸ˜‚ #vtep\n",
      "RT @sanaa___kg: Mdrrr la tÃªte de Ryad ğŸ˜‚ğŸ˜‚#VTEP\n",
      "RT @ophelieavgc: pauvre @justriadh ğŸ˜‚ #VTEP\n",
      "RT @YNadoore: Riadh dans #VTEP ğŸ˜­\n",
      "RT @Mariettegalaxy: je suis deadğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ #VTEP\n",
      "RT @Rodrigu64581700: Vous Ãªtes mÃ©chants le pauvre  ğŸ˜‚ğŸ˜‚ğŸ˜‚ #VTEP\n",
      "RT @shaqbrewster: .@BernieSanders had to move his South Carolina event indoors due to bad weather, so he addressed an overflow crowd outsidâ€¦\n",
      "RT @Jennifer1382: L'Ã©preuve du mime en apesanteur est absolument gÃ©niale, il faut la garder ğŸ˜ #VTEP\n",
      "RT @mauritiusgirly: C'est pas facile de faire des mimes suspendu ğŸ˜‚ğŸ˜‚ le dÃ©lire  #VTEP\n",
      "RT @soldatguerrier1: excellent #VTEP ce soir merci pour toutes ses nouvelles Ã©preuves @Arthur_Officiel. @VTEP_TF1 #TF1\n",
      "RT @Rodrigu64581700: Mais qui invente ces Ã©preuves ğŸ˜‚ğŸ˜‚ğŸ˜‚ je l'embrasse ğŸ˜˜ğŸ˜˜ #vtep ğŸ™\n"
     ]
    }
   ],
   "source": [
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in tweepy.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    status_list.append(status._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a streaming listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('python.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch a real time listener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['#python'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n"
     ]
    }
   ],
   "source": [
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in status_list:\n",
    "    tokens = preprocess(status['text'])\n",
    "    tokens_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@rocmaelias',\n",
       " ':',\n",
       " 'Jvien',\n",
       " 'de',\n",
       " 'dÃ©couvrir',\n",
       " 'que',\n",
       " 'le',\n",
       " 'ptit',\n",
       " 'truc',\n",
       " 'jaune',\n",
       " \"c'est\",\n",
       " 'comme',\n",
       " 'le',\n",
       " 'jaune',\n",
       " 'de',\n",
       " \"l'oeuf\",\n",
       " 'https://t.co/UNDANnNc6y']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Term Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('french') + punctuation + ['RT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count term occurences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('jaune', 2), ('Jvien', 1), ('dÃ©couvrir', 1), ('ptit', 1), ('truc', 1)]\n",
      "[('jaune', 2), ('Jvien', 1), ('dÃ©couvrir', 1), ('ptit', 1), ('truc', 1)]\n",
      "[('jaune', 2), ('Jvien', 1), ('dÃ©couvrir', 1), ('ptit', 1), ('truc', 1)]\n",
      "[('jaune', 2), ('veux', 2), ('Jvien', 1), ('dÃ©couvrir', 1), ('ptit', 1)]\n",
      "[('jaune', 2), ('veux', 2), ('Jvien', 1), ('dÃ©couvrir', 1), ('ptit', 1)]\n",
      "[('ğŸ˜…', 3), ('jaune', 2), ('veux', 2), ('nouvelles', 2), ('Jvien', 1)]\n",
      "[('ğŸ˜…', 3), ('jaune', 2), (\"c'est\", 2), ('Je', 2), ('veux', 2)]\n",
      "[('â¤', 6), ('ğŸ˜…', 3), ('jaune', 2), (\"c'est\", 2), ('Je', 2)]\n",
      "[('â¤', 6), ('ğŸ¤£', 3), ('ğŸ˜…', 3), ('jaune', 2), (\"c'est\", 2)]\n",
      "[('â¤', 6), ('ğŸ˜‚', 6), ('ğŸ¤£', 3), ('ğŸ˜…', 3), ('jaune', 2)]\n"
     ]
    }
   ],
   "source": [
    "count_all = Counter()\n",
    "for status in status_list:\n",
    "    \n",
    "    # Create a list with all the terms\n",
    "    terms_stop = [term for term in preprocess(status['text']) if term not in stop and not term.startswith(('#', '@'))]\n",
    "    \n",
    "    terms_hash = [term for term in preprocess(status['text']) if term.startswith('#')]\n",
    "    \n",
    "    # Update the counter\n",
    "    count_all.update(terms_stop)\n",
    "    \n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_bigram = bigrams(terms_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"L'accident\", 'Ã§a'), ('Ã§a', 'fera'), ('fera', 'peu'), ('peu', 'buzz'), ('buzz', 'ğŸ˜‚'), ('ğŸ˜‚', 'ğŸ˜‚'), ('ğŸ˜‚', 'ğŸ˜‚'), ('ğŸ˜‚', \"t'es\"), (\"t'es\", 'enfoir'), ('enfoir', 'Ã©'), ('Ã©', 'ğŸ˜‚'), ('ğŸ˜‚', 'ğŸ˜‚'), ('ğŸ˜‚', 'ğŸ˜‚')]\n"
     ]
    }
   ],
   "source": [
    "print(list(terms_bigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Term Co-occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will stream recents tweets about this last European Champion's Leagues matchs played this week.\n",
    "So, will use the stream function to stream the #UCL ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch a real time listener on #UCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '#UCL'\n",
    "max_tweets = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_tweets = [status for status in tweepy.Cursor(api.search, q=query).items(max_tweets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save tweets in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ucl_tweets.json', 'w') as outfile:\n",
    "    for status in searched_tweets:\n",
    "        json.dump(status._json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "searched_tweets[0]._json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import english stop word and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('french') + punctuation + ['RT', 'ï¸', 'â€¦', '', ] + stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count term occurences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all = Counter()\n",
    "for status in searched_tweets:\n",
    "    \n",
    "    # Create a list with all the terms\n",
    "    terms_stop = [term for term in preprocess(status._json['text']) if term not in stop and not term.startswith(('#', '@')) and not term.isdigit()]\n",
    "    \n",
    "    # Update the counter\n",
    "    count_all.update(terms_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Messi', 328), ('âš½', 304), ('ğŸ†', 243), ('Week', 155), ('Goal', 135), ('Sterling', 130), ('Lionel', 126), ('David', 126), ('Neres', 125), ('Raheem', 124), ('Philippe', 124), ('Coutinho', 124), ('ğŸ¤”', 117), ('Agree', 117), ('ord', 117), ('ğŸ¥‡', 98), ('ğŸ—“', 93), ('Majestic', 92), ('scoops', 92), ('prize', 92), ('https://t.co/L2qzAL39wg', 92), ('ğŸ˜‚', 88), ('\\U0001f929', 84), ('us', 81), ('â€™', 81), ('If', 80), ('nowt', 78), ('else', 78), ('given', 78), ('priceless', 78), ('moments', 78), ('How', 78), ('https://t.co/pChlaUsJG4', 78), ('Redondo', 77), ('ğŸ˜', 76), ('amp', 68), ('best', 65), ('assists', 64), ('history', 64), ('ğŸ”¥', 64)]\n"
     ]
    }
   ],
   "source": [
    "# Print the most frequent words\n",
    "print(count_all.most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all = Counter()\n",
    "for status in searched_tweets:\n",
    "    \n",
    "    # Create a list with all the terms\n",
    "    terms_hash = [term for term in preprocess(status._json['text']) if term not in stop and term.startswith(('#')) and not term.isdigit()]\n",
    "    \n",
    "    # Update the counter\n",
    "    count_all.update(terms_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#UCL', 696), ('#POTW', 99), ('#OnThisDay', 85), ('#VAR', 80), ('#FlashbackFriday', 51), ('#COYS', 40), ('#GOTW', 23), ('#ajabay', 20), ('#LFC', 19), ('#FCPorto', 17), ('#SomosPorto', 17), ('#Ajax', 17), ('#TalDÃ­aComoHoy', 16), ('#juvaja', 11), ('#MCITOT', 11), ('#ã‚¦ã‚¤ã‚¤ãƒ¬', 8), ('#ã‚¦ã‚¤ã‚¤ãƒ¬ã‚¢ãƒ—ãƒª', 8), ('#ChampionsLeagu', 8), ('#ucl', 8), ('#UEL', 6), ('#MUFC', 6), ('#JuveAjax', 6), ('#OTD', 4), ('#ChampionsLeague', 4), ('#TICKETMANIA', 4), ('#Datos', 4), ('#ChampionsLea', 4), ('#Messi', 3), ('#FIFA19', 3), ('#Å kriniar', 2), ('#Eintracht', 2), ('#totaja', 2), ('#LdC', 2), ('#YNWA', 2), ('#GETREADY', 2), ('#LaLiga', 2), ('#DrakeCurse', 2), ('#barcelona', 2), ('#Guardiola', 2), ('#ajajuv', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Print the most frequent hashtags\n",
    "print(count_all.most_common(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Term co-occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = defaultdict(lambda : defaultdict(int))\n",
    " \n",
    "# f is the file pointer to the JSON data set\n",
    "for status in searched_tweets: \n",
    "    \n",
    "    terms_only = [term for term in preprocess(status._json['text']) \n",
    "                  if term not in stop \n",
    "                  and not term.startswith(('#', '@'))\n",
    "                  and not term.isdigit()]\n",
    " \n",
    "    # Build co-occurrence matrix\n",
    "    for i in range(len(terms_only)-1):            \n",
    "        for j in range(i+1, len(terms_only)):\n",
    "            w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "            if w1 != w2:\n",
    "                com[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_max = []\n",
    "# For each term, look for the most common co-occurrent terms\n",
    "for t1 in com:\n",
    "    t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    for t2, t2_count in t1_max_terms:\n",
    "        com_max.append(((t1, t2), t2_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('âš½', 'ğŸ†'), 472), (('Messi', 'âš½'), 271), (('âš½', 'ğŸ¤”'), 234), (('Goal', 'âš½'), 234), (('Goal', 'ğŸ†'), 234)]\n"
     ]
    }
   ],
   "source": [
    "# Get the most frequent co-occurrences\n",
    "terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "print(terms_max[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate a term co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word = \"Messi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_search = Counter()\n",
    "for status in searched_tweets:\n",
    "    terms_only = [term for term in preprocess(status._json['text']) \n",
    "                  if term not in stop \n",
    "                  and not term.startswith(('#', '@'))\n",
    "                  and not term.isdigit()]\n",
    "    if search_word in terms_only:\n",
    "        count_search.update(terms_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence for Messi:\n",
      "[('Messi', 328), ('âš½', 271), ('ğŸ†', 234), ('Week', 152), ('Goal', 135), ('Lionel', 126), ('Raheem', 124), ('Sterling', 124), ('David', 124), ('Neres', 124), ('Philippe', 124), ('Coutinho', 124), ('ğŸ¤”', 117), ('Agree', 117), ('ord', 117), ('ğŸ¥‡', 98), ('Majestic', 92), ('scoops', 92), ('prize', 92), ('https://t.co/L2qzAL39wg', 92)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Co-occurrence for %s:\" % search_word)\n",
    "print(count_search.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Visualisation Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a free cell to remove after"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
